两者思路确实一致：都是借用“Cost-Effective Adversarial Attacks against Scene Text Recognition”里基于 margin/step 的序列级框架，只不过把图像 → 文本识别模型换成了 CTC-based wav2vec2。但是语音识别有几个不同点，需要特别注意：

1. CTC 解码的“塌缩”特性  
   – 虽然我们只要**一帧**被打破就视为攻击成功，但 CTC 的 greedy 或 beam-search 解码会先 collapse 重复 token、再去掉 blank，这一帧的错误很可能在最终序列里被忽略，不会影响转录结果。  
   – 换言之，`min_f d_f ≤ 0` 并不保证“最终文本”发生改变。

2. 面向“文本”级别的失效及改进  
   a. 用 CTC 对抗目标替代 frame-wise margin：  
      - 直接最大化 `– log p(orig transcript | x + δ)`（CTC 负对数似然），或者最小化 `log p(orig transcript) – log p(adv transcript)`，针对整个序列进行优化。  
   b. 加入 edit-distance 损失：  
      - 在训练过程中用强化学习或差分可导的编辑距离近似（如 soft-WER）把“序列距离”纳入目标。  
   c. 结合 beam-search 得分：  
      - 在每次前向选取 beam top-k，计算它们与原始转录的概率差，目标是让任意一个“非原始”序列分数超出原始分数。

3. 建议  
   - **快速试验**：先把 margin 换成 CTC loss 差分，看看对最终文本的扰动影响更明显。  
   - **多方案融合**：在最终 loss 里加一个 `β * CTC_NLL(orig)` 项，或直接用 targeted CTC loss（把最终 decode 强制推向一个随机／特定的错误序列）。  
   - **Beam-search 引导**：在每次迭代后跑 beam-search 解码，记录 top-k 文本的概率和 edit 距离，把这些信号反馈到 loss 里。

总结：  
- 现在的 frame-wise margin+step 会在“中间特征”层面击破最弱帧，但并不一定能改写 CTC 后的最终输出。  
- 要提升成功率，需要把**“序列级 CTC 目标”**或**“文本编辑距离”** 整合到 loss 里。


---

```markdown
那为什么这个 margin loss 的策略在图像识别中是有用的，难道在那里不存在像这样的帧攻击成功但是最终序列中被忽略导致转录结果没有被影响的问题吗
```

在图像–文本识别（scene text）里，margin‐based 的序列级攻击能很可靠地破坏最终输出，主要有两个原因：

1. 在那种模型里，每个时间（或空间）位置的预测往往直接对应一个字符，几乎不存在「重复帧」的概念，模型把卷积／LSTM 的每一格输出都当成一个新字符来解码。  
2. 因此，只要某一个位置的 margin dₖ 变负，就会立刻导致最终解码序列中对应字符发生变化。

但在 CTC‐based 的 ASR 里：
- CTC 解码（无论 greedy 还是 beam‐search）会先 collapse 重复的 token，再去掉 blank。  
- 如果你在一串 “EEE” 中把中间那个“E”的 logit margin 打破（dᵢ≤0），CTC collapse 后仍然可能输出 “E”，并不会影响最终文本。

也就是说，**一帧攻击成功** (frame‐level margin ≤0) 并不等同于**一个符号被删除或替换**，最终序列可能**完全不变**。

要解决这个问题，有两种思路：

1. **对齐后再 collapse，再做 margin 计算**  
   – 先用 greedy 对齐得到 labels y_f，然后做一次 run‐length encoding，把重复的 frame 聚成一个符号：  
     ```python
     # labels: [T], e.g. [E,E,E,B,B,C,C]
     idx = [0, 3, 5]  # 每个新符号第一次出现的位置
     labels_eff = labels[idx]     # [E, B, C]
     logits_eff = logits[idx]     # [T_eff, C]
     # 再对 logits_eff 计算 margin_loss
     ```
   – 这样保证每个 margin 分量都代表最终输出序列的一个字符，破坏它就一定能改变文本。

2. **直接用 CTC 序列级目标**  
   – 将 loss 从 frame‐wise margin 换成真正的 CTC 负对数似然（或它和-edit‐distance 的组合），例如：  
     ```python
     ctc_nll = CTC_loss(logits.unsqueeze(0).log_softmax(-1),
                        orig_ids.unsqueeze(0),
                        input_lengths=[T],
                        target_lengths=[len(orig_ids)])
     loss = L2 + c * ctc_nll
     ```  
   – 或者在 margin+step 基础上再加一个“soft‐WER”项、beam‐search top‐k 差分等，让损失直接反映最终文本输出的变化。
   (直接用 CTC 会不会像另一篇论文说的一样收敛困难？)

**总结**  
- margin‐step 在图像/scene‐text 上很直接，因为“breaking one frame = changing one字符”。  
- 在 CTC-ASR 上，需要先 collapse 重复帧或干脆用 CTC‐level loss，才能让“打破 margin” 真正映射到“目标文本变化”。  

你可以先在 `loss_sh.py` 里加上「collapse 后再 margin」的逻辑，验证第一步效果；后续再尝试用 CTC‐NLL 或 edit‐distance 进一步提升成功率。


以及即使这样的攻击成功，例如只是攻击成功了一个字符，是不是意义并不是很大，例如这是一个语音输入，这个指令仍然能够被执行？
因为语义上并没有太大的变化？(这一步是不是可以去实际验证一下，比如给出一些命令，以及扰动之后的，让实际的板块去执行)
或者因为这里存在着前后文，对抗性可能更强

而原文的图像识别只是识别一个图片？


不过现在还是先做出来这样 untarget 攻击吧


中文会不会效果就没有那么好，因为这时候是修改字，但是中文的组词相对更容易猜测一些，所以攻击后的实际效果没那么好


能不能依据一个多解性指标来说明我们攻击的效果